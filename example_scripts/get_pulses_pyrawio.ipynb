{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a50ec62-e2f8-4e89-adec-a938d66281a0",
   "metadata": {},
   "source": [
    "**Settings:**\n",
    "Here are some settings that I put at the top of all my notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8eeed28-3f91-4740-815f-57e2384c8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we may need some code in the ../python directory and/or matplotlib styles\n",
    "import sys\n",
    "sys.path.append('../python/')\n",
    "\n",
    "#matplotlib for plotting\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('../mplstyles/stylelib/standard.mplstyle')\n",
    "\n",
    "#other computational libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.constants as co\n",
    "import scipy.stats as ss\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6fb936-c62f-45c4-a680-994c1ecd4470",
   "metadata": {},
   "source": [
    "Getting Pulses Using `pyRawIO` \n",
    "==============================\n",
    "\n",
    "The SuperCDMS library `pyRawIO` allows us to read raw MIDAS DAQ files directly to get the raw pulses that our analysis RQs are based on. The library is hosted on the GitLab page for SuperCDMS [[GitLab:pyRawIO][GitLab:pyRawIO]]. The library `pyRawIO` depends on other libraries like `CDMSbats` and `ROOT`-- those are sometimes difficult to install so these instructions focus on using a `singularity` container [[Singularity][Singularity]] where all of that stuff is already pre-installed and all you need to run this is to install `singularity` and have the image files (which are maintained by SuperCDMS [[Singularity-Images][Singularity-Images]]). \n",
    "\n",
    "[GitLab:pyRawIO]: https://gitlab.com/supercdms/DataHandling/pyRawIO \"GitLab:pyRawIO\"\n",
    "[Singularity]: https://docs.sylabs.io/guides/latest/user-guide/introduction.html \"Singularity\" \n",
    "[Singularity-Images]: https://confluence.slac.stanford.edu/display/CDMS/Using+Containerized+CDMS+Offline+Releases#UsingContainerizedCDMSOfflineReleases-WheretogettheSingularityimagesfrom? \"SCDMS Singularity Image Registry\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5141d-913c-48f3-99ce-53fe8c8cf24f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Singularity\n",
    "-----------\n",
    "\n",
    "To begin using singularity you will probably have to install it, but test if you have it by typing `singularity --help` in the terminal; if that returns the help menu then you already have it! Singularity depends on go [[go][go]], so if you don't have it you will have to install that first. Install `singularity` and `go` according to the instructions in this repository [[example_scripts/singularity][sing-install]].\n",
    "\n",
    "Here are some useful singularity images on disk on the UMN cluster:\n",
    "\n",
    " - `/data/chocula/villaa/cdmsfull_latest.sif` (don't remember exactly the SuperCDMS release tag, V04-12?)\n",
    " - `/data/chocula/villaa/cdmsfull_V04-07-00.sif`  \n",
    " - `/data/chocula/villaa/cdmsfull_V04-13-00.sif`\n",
    " \n",
    "Before using `singularity` you should have the correct environmental variables set, this could be:\n",
    "\n",
    "```\n",
    "export GOCACHE=\"/data/chocula/villaa/.cache/go\"\n",
    "export GOMODCACHE=\"/data/chocula/villaa/.cache/go/pkg/mod\"\n",
    "export GOPATH=\"/data/chocula/villaa/go\"\n",
    "export CCACHE_DIR=\"/data/chocula/villaa/.ccache\"\n",
    "export PATH=$PATH:/data/chocula/villaa/goinstall/go/bin\n",
    "export SINGULARITY_BINDPATH=/data\n",
    "```\n",
    "\n",
    "Most of these set up the paths and cache directories for the language `go` itself. Make the cache directories something that has some space. On the UMN cluster my home directory is severely limited so I used the data directory `/data/chocula/villaa`, other users should be able to make their own directory in `/data/chocula` if they don't already have one. Contact Anthony Villano to get help with setting that up if you don't have permission. \n",
    "\n",
    "Open a shell in `singularity` then you will have access to the code environments above created for SuperCDMS and having `pyRawIO`, `CDMSbats` and `ROOT` already installed.\n",
    "\n",
    "```\n",
    "$ singularity shell /data/chocula/villaa/cdmsfull_V04-13-00.sif \n",
    "WARNING: DEPRECATED USAGE: Environment variable SINGULARITY_BINDPATH will not be supported in the future, use APPTAINER_BINDPATH instead\n",
    "Singularity> \n",
    "```\n",
    "\n",
    "We also have the ability to run a Jupyter Labs server inside the `singularity` container and expose a web port that can be accessed by a browser outside. Many users typically work from remote locations, so below is the procedure for getting a singularity container running with a Jupyter Labs server and accessing it from a web browser. \n",
    "\n",
    " 1. Connect to UMN via VPN\n",
    " 2. ssh to a UMN machine (maybe `cdms2.spa.umn.edu`) and map port 8889 to your localhost\n",
    " 3. Spin up a `singularity` instance with a Jupyter Labs server running on port 8889 (this port should not conflict with any running Jupyter Lab instances on the UMN machine or your local machine)\n",
    " 4. Open a web browser on your local machine with the address `https://localhost:8889`\n",
    " \n",
    " The first step is accomplished through your VPN client and two-factor authentication as normal. The second step can be accomplished (once the VPN connection is established) via this command on your localhost:\n",
    " \n",
    " ```\n",
    " $ ssh -L 8889:localhost:8889 user@cdms2.spa.umn.edu\n",
    " ```\n",
    "\n",
    "The third step is accomplished in the newly opened shell to the UMN computer `cdms2.spa.umn.edu`. Use the following command:\n",
    "\n",
    "```\n",
    "$ singularity run /data/chocula/villaa/cdmsfull_V04-13-00.sif --no-browser --port=8889\n",
    "```\n",
    "\n",
    "After this step the terminal will display status updates for the `singularity` container that is running. You can check there to see if there are problems or if it's stalled (hit enter and you should see a new line if it's still running). The main thing that can stall the `singularity` instance is the VPN or other connections dropping. If it is the first time you've run this command you will see some output after it that gives you an authentication key to get into the Jupyter Lab instance for the first time (step 4). \n",
    "\n",
    "**Note:** if your connection breaks down at some point and the terminal stops responding it is likely that you _have not_ stopped your Jupyter Lab instance. You should be able to simply re-do the second step above. You will no longer have the `singularity`/Jupyter status updates in the terminal but you should be able to connect to and use the Jupyter instance as in step 4. In other words in the case of a broken connection where you want to re-connect to your previous Jupyter instance just do steps 2 and 4 again. \n",
    "\n",
    "If you would like to find and remove previous running instances of Jupyter Lab do:\n",
    "\n",
    "```\n",
    "$ ps -u user |grep jupyter-lab\n",
    "```\n",
    "\n",
    "Then kill all the processes you find like:\n",
    "\n",
    "```\n",
    "$ kill -9 PID\n",
    "```\n",
    "\n",
    "[go]: https://go.dev/doc/install \"GO programming language\"\n",
    "[sing-install]: https://github.com/villano-lab/R76Analysis/tree/master/example_scripts/singularity \"Installing Singularity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729d7e52-23fc-4b65-b1d7-886161eaf4e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Loading In Events\n",
    "-----------------\n",
    "\n",
    "The toolchain that was developed involves getting a text-file list of series/event numbers from our processed data. That list will be passed to the script `singularity_pulseSave.py` and it will produce a file called `pulses.pkl` that contains a data structure with all the requested pulses in it. What I (Anthony) have used in the past is a formatted file from Matt that shows various analysis quantities for events, then I select a subset of those to get the pulses for. **Note:** A set of around 1200 pulses will result in an output file from `singularity_pulseSave.py` of about 100 Mb. \n",
    "\n",
    "The formatted file from Matt looks like:\n",
    "\n",
    "```\n",
    "****************************************************************************************************************************************************\n",
    "*    Row   *   SeriesNumber *    EventNumber *       pt_keVee *        coinwin *      PTOFdelay *   zip4.PAWKmax *   zip4.PAWKr50 *         cchisq *\n",
    "****************************************************************************************************************************************************\n",
    "*     6107 *    72209162200 *          70265 * 0.035492627470 *              0 *       5.92e-05 * 8.24276502e-07 * 0.000400652451 *              1 *\n",
    "*    40145 *    72209162200 *         430871 * 0.099516752413 *              1 *        1.6e-05 * 8.27113601e-07 * 0.000405632437 *              1 *\n",
    "*    40417 *    72209162200 *         440254 * 0.045830295421 *              0 *       3.92e-05 * 8.10276066e-07 * 0.000405238078 *              0 *\n",
    "*    44615 *    72209162200 *         480783 * 0.036226841102 *              0 *       3.44e-05 * 8.13807932e-07 * 0.000405366383 *              1 *\n",
    "*    45533 *    72209162200 *         490759 * 0.260436379674 *              0 *       3.44e-05 * 8.25049970e-07 * 0.000405464026 *              1 *\n",
    "*    48298 *    72209162200 *         520788 * 0.035015524687 *              0 *       4.48e-05 * 8.34448512e-07 * 0.000405338227 *              1 *\n",
    "*    54136 *    72209162200 *         590105 * 0.505873941394 *              0 *       4.64e-05 * 8.16696716e-07 * 0.000405465675 *              1 *\n",
    "*    55791 *    72209162200 *         600805 * 0.085282336319 *              0 *       5.84e-05 * 8.34338783e-07 * 0.000405886791 *              1 *\n",
    "*    60164 *    72209162200 *         650562 * 0.035800719635 *              0 *       5.36e-05 * 8.32320110e-07 * 0.000405415123 *              1 *\n",
    "*    64728 *    72209162200 *         700360 * 0.036148316644 *              0 *        4.8e-05 * 8.01631662e-07 * 0.000405576181 *              1 *\n",
    "*    73880 *    72209162200 *         800237 * 0.040017307227 *              1 *       1.28e-05 * 8.03877571e-07 * 0.000405295972 *              1 *\n",
    "*    84589 *    72209162200 *         910814 * 0.039486779722 *              0 *        2.4e-06 * 8.17602531e-07 * 0.000405210718 *              1 *\n",
    "...\n",
    "```\n",
    "\n",
    "This is the head of the file in this repository `coin_analysis/data/r76_roi_events_trig_set1.txt`. The code below will read in three such files for triggered events in this repository. It puts the information in a `Pandas` DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8136189-bfeb-40d5-834b-aa10191cd44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   garbage    Row  SeriesNumber  EventNumber  pt_keVee  coinwin  PTOFdelay  \\\n",
      "0      NaN   6107   72209162200        70265  0.035493        0   0.000059   \n",
      "1      NaN  40145   72209162200       430871  0.099517        1   0.000016   \n",
      "2      NaN  40417   72209162200       440254  0.045830        0   0.000039   \n",
      "3      NaN  44615   72209162200       480783  0.036227        0   0.000034   \n",
      "4      NaN  45533   72209162200       490759  0.260436        0   0.000034   \n",
      "5      NaN  48298   72209162200       520788  0.035016        0   0.000045   \n",
      "6      NaN  54136   72209162200       590105  0.505874        0   0.000046   \n",
      "7      NaN  55791   72209162200       600805  0.085282        0   0.000058   \n",
      "8      NaN  60164   72209162200       650562  0.035801        0   0.000054   \n",
      "9      NaN  64728   72209162200       700360  0.036148        0   0.000048   \n",
      "\n",
      "   zip4.PAWKmax  zip4.PAWKr50  cchisq  \n",
      "0  8.242765e-07      0.000401       1  \n",
      "1  8.271136e-07      0.000406       1  \n",
      "2  8.102761e-07      0.000405       0  \n",
      "3  8.138079e-07      0.000405       1  \n",
      "4  8.250500e-07      0.000405       1  \n",
      "5  8.344485e-07      0.000405       1  \n",
      "6  8.166967e-07      0.000405       1  \n",
      "7  8.343388e-07      0.000406       1  \n",
      "8  8.323201e-07      0.000405       1  \n",
      "9  8.016317e-07      0.000406       1  \n"
     ]
    }
   ],
   "source": [
    "#fetch the data from the file from Matt formatted as above:\n",
    "\n",
    "coindata1 = pd.read_csv(\"../coin_analysis/data/r76_roi_events_trig_set1.txt\", skiprows=3, engine='python',skipfooter=1,sep=\"*\",skipinitialspace=True, \\\n",
    "            index_col=False, \\\n",
    "            names=['garbage','Row','SeriesNumber','EventNumber','pt_keVee','coinwin','PTOFdelay','zip4.PAWKmax','zip4.PAWKr50','cchisq'], \\\n",
    "            delim_whitespace=False)\n",
    "coindata2 = pd.read_csv(\"../coin_analysis/data/r76_roi_events_trig_set2.txt\", skiprows=3, engine='python',skipfooter=1,sep=\"*\",skipinitialspace=True, \\\n",
    "            index_col=False, \\\n",
    "            names=['garbage','Row','SeriesNumber','EventNumber','pt_keVee','coinwin','PTOFdelay','zip4.PAWKmax','zip4.PAWKr50','cchisq'], \\\n",
    "            delim_whitespace=False)\n",
    "coindata3 = pd.read_csv(\"../coin_analysis/data/r76_roi_events_trig_set3.txt\", skiprows=3, engine='python',skipfooter=1,sep=\"*\",skipinitialspace=True, \\\n",
    "            index_col=False, \\\n",
    "            names=['garbage','Row','SeriesNumber','EventNumber','pt_keVee','coinwin','PTOFdelay','zip4.PAWKmax','zip4.PAWKr50','cchisq'], \\\n",
    "            delim_whitespace=False)\n",
    "coindata4 = pd.read_csv(\"../coin_analysis/data/r76_roi_events_trig_set4.txt\", skiprows=3, engine='python',skipfooter=1,sep=\"*\",skipinitialspace=True, \\\n",
    "            index_col=False, \\\n",
    "            names=['garbage','Row','SeriesNumber','EventNumber','pt_keVee','coinwin','PTOFdelay','zip4.PAWKmax','zip4.PAWKr50','cchisq'], \\\n",
    "            delim_whitespace=False)\n",
    "frames = [coindata1, coindata2, coindata3, coindata4]\n",
    "\n",
    "\n",
    "coindata = pd.concat(frames)\n",
    "print (coindata.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fbc5c-5e6e-4e76-96c2-271fdb23af5b",
   "metadata": {},
   "source": [
    "Once I have this data in a `Pandas` DataFrame I can use `numpy` arrays to select the Series/Event numbers I want based on some criteria. Below I select all of the events that have a good or bad `cchisq` (Chisquare cut) and energy over 100 eV. The slicing can be done in the `Pandas` DataFrame directly, but I prefer `numpy` arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b308bcee-a5d5-429c-bad3-54d421543147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 good chisquare events\n",
      "141 bad chisquare events\n"
     ]
    }
   ],
   "source": [
    "#put relevant RQs data into numpy arrays\n",
    "series = np.asarray(coindata['SeriesNumber'])\n",
    "ev = np.asarray(coindata['EventNumber'])\n",
    "EkeV = np.asarray(coindata['pt_keVee'])\n",
    "ccoinwin = np.asarray(coindata['coinwin'],dtype=np.dtype(bool))\n",
    "cchisq = np.asarray(coindata['cchisq'],dtype=np.dtype(bool))\n",
    "\n",
    "#get cut for good/bad chisquare\n",
    "csiggood = (EkeV>0.1)&cchisq\n",
    "csigbad  = (EkeV>0.1)&~cchisq\n",
    "\n",
    "#print out number of each events\n",
    "print('{} good chisquare events'.format(np.sum(csiggood)))\n",
    "print('{} bad chisquare events'.format(np.sum(csigbad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe1825-bca5-487b-8bc9-5960708c1268",
   "metadata": {},
   "source": [
    "Then I place, for example, the good Chisquare events into a text file with simply the `SeriesNumber` and `EventNumber` RQs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a162637-c141-483d-ab7d-d039e647a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('goodchi.txt', 'w') as f:\n",
    "    for i,s in enumerate(series):\n",
    "        if(csiggood[i]):\n",
    "          f.write('{} {}\\n'.format(s,ev[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566db65-e095-4c77-bae4-69ceaf0b56ea",
   "metadata": {},
   "source": [
    "This file looks like this:\n",
    "\n",
    "```\n",
    "72209162200 490759\n",
    "72209162200 590105\n",
    "72209162200 970769\n",
    "72209162200 990193\n",
    "72209162200 1200805\n",
    "72209162200 1920692\n",
    "72209162200 2640676\n",
    "72209162200 3320285\n",
    "72209162200 3470461\n",
    "72209162200 5310198\n",
    "72209171225 1500320\n",
    "72209171225 1970453\n",
    "72209171225 2280315\n",
    "72209171225 2920809\n",
    "72209171225 3240861\n",
    "72209171225 4530720\n",
    "...\n",
    "```\n",
    "\n",
    "It will be used as input to the `singularity_pulseSave.py` script to fetch the raw pulses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7739072-c746-4224-965e-44e43b584a7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "`singularity_pulseSave.py`\n",
    "-------------------------\n",
    "\n",
    "This script **must** be run on a machine where the data is mounted within the `singularity` container. I typically use `cdms2.spa.umn.edu` and connect to it by VPN/ssh. \n",
    "\n",
    " 1. Spin up a `singularity` shell at UMN or wherever has access to the raw data\n",
    " 2. Go into the `python` directory of this repository\n",
    " 3. Execute the `singularity_pulseSave.py`\n",
    " 4. Copy the output `pulses.pkl` to wherever you want to read the pulses from could be through the UMN/singularity Jupyter or could be on your local machine. You should only need the `pickle` Python library to read the pulses if you have this file. \n",
    " \n",
    " \n",
    " For Step 1, I do two sub-steps. First, connect to the UMN cluster (I do not have raw data locally):\n",
    " \n",
    " ```\n",
    " $ ssh avillano@cdms2.spa.umn.edu\n",
    " ```\n",
    " \n",
    " Then I start up a singularity container like:\n",
    " \n",
    " ```\n",
    " $ singularity shell /data/chocula/villaa/cdmsfull_V04-13-00.sif \n",
    " ```\n",
    " \n",
    " I then make sure I have a checked-out version of the `R76Analysis` repository on the UMN computers and move into the `python` directory, that's Step 2. \n",
    " \n",
    " In the `singularity` shell I then execute the following command to invoke the `singularity_pulseSave.py` script:\n",
    " \n",
    " ```\n",
    " Singularity> python3 singularity_pulseSave.py --evlist=../example_scripts/goodchi.txt \n",
    " ```\n",
    " \n",
    " **Note:** this input file is the text file that was made in the previous section, containing all events over 100 eV passing the Chisquare cut. \n",
    " \n",
    " The output of this command will be lots of text to the terminal window and a single file in the `python` directory called `pulses.pkl` for Step 4, move that file to the repository directory `example_scripts` (the directory where this note sits). \n",
    " \n",
    " ```\n",
    " Singularity> mv pulses.pkl ../example_scripts/\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e8a47-2375-49e9-8981-4d91bbe96ead",
   "metadata": {},
   "source": [
    "Reading Back Events\n",
    "-------------------\n",
    "\n",
    "Having the `pulses.pkl` file and the `goodchi.txt` file in our working directory (`example_scripts`), we can now look at the pulses that we've fetched alongside the RQ data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6661ec-378e-4236-b860-23e48fc9ff8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fano_pub_new)",
   "language": "python",
   "name": "fano_pub_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
